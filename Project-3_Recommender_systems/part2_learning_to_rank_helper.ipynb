{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Load the dataset for one fold\n",
    "def load_one_fold(data_path):\n",
    "    X_train, y_train, qid_train = load_svmlight_file(str(data_path + 'train.txt'), query_id=True)\n",
    "    X_test, y_test, qid_test = load_svmlight_file(str(data_path + 'test.txt'), query_id=True)\n",
    "    y_train = y_train.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    _, group_train = np.unique(qid_train, return_counts=True)\n",
    "    _, group_test = np.unique(qid_test, return_counts=True)\n",
    "    return X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test\n",
    "\n",
    "def ndcg_single_query(y_score, y_true, k):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "# calculate NDCG score given a trained model \n",
    "def compute_ndcg_all(model, X_test, y_test, qids_test, k=10):\n",
    "    unique_qids = np.unique(qids_test)\n",
    "    ndcg_ = list()\n",
    "    for i, qid in enumerate(unique_qids):\n",
    "        y = y_test[qids_test == qid]\n",
    "\n",
    "        if np.sum(y) == 0:\n",
    "            continue\n",
    "\n",
    "        p = model.predict(X_test[qids_test == qid])\n",
    "\n",
    "        idcg = ndcg_single_query(y, y, k=k)\n",
    "        ndcg_.append(ndcg_single_query(p, y, k=k) / idcg)\n",
    "    return np.mean(ndcg_)\n",
    "\n",
    "# get importance of features\n",
    "def get_feature_importance(model, importance_type='gain'):\n",
    "    return model.feature_importance(importance_type=importance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique queries: 10000\n",
      "\n",
      "Distribution of relevance labels:\n",
      "Relevance label 0.0: 624263 samples\n",
      "Relevance label 1.0: 386280 samples\n",
      "Relevance label 2.0: 159451 samples\n",
      "Relevance label 3.0: 21317 samples\n",
      "Relevance label 4.0: 8881 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "web10k_data_path = \"MSLR-WEB10K/Fold1/\"\n",
    "\n",
    "# Load the Web10k dataset for one fold\n",
    "X_train, y_train, qid_train = load_svmlight_file(str(web10k_data_path + 'train.txt'), query_id=True)\n",
    "X_test, y_test, qid_test = load_svmlight_file(str(web10k_data_path + 'test.txt'), query_id=True)\n",
    "X_vali, y_vali, qid_vali = load_svmlight_file(str(web10k_data_path + 'vali.txt'), query_id=True)\n",
    "\n",
    "# Print the number of unique queries in total\n",
    "total_unique_queries = len(set(np.concatenate((qid_train, qid_test,qid_vali))))\n",
    "print(f\"Total number of unique queries: {total_unique_queries}\")\n",
    "\n",
    "# Show the distribution of relevance labels\n",
    "unique_labels, label_counts = np.unique(np.concatenate((y_train, y_test,y_vali)), return_counts=True)\n",
    "print(\"\\nDistribution of relevance labels:\")\n",
    "for label, count in zip(unique_labels, label_counts):\n",
    "    print(f\"Relevance label {label}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 13\n",
    "\n",
    "Total number of unique queries (including vali.txt) = 10,000 \n",
    "\n",
    "Excluding vali.txt, Total number of unique queries = 8,000\n",
    "\n",
    "Distribution of relevance labels (*including vali.txt* of Fold1):\n",
    "\n",
    "| Relevance Label | Number of Samples |\n",
    "|------------------|-------------------|\n",
    "| 0.0              | 624263            |\n",
    "| 1.0              | 386280            |\n",
    "| 2.0              | 159451            |\n",
    "| 3.0              | 21317             |\n",
    "| 4.0              | 8881              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for MSLR-WEB10K/Fold1/\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25637\n",
      "[LightGBM] [Info] Number of data points in the train set: 723412, number of used features: 136\n",
      "#################################  EVALUATING MODEL-FOLD 1   #################################\n",
      "nDCG@3: 0.4564571300800643\n",
      "nDCG@5: 0.4632890672260867\n",
      "nDCG@10: 0.48286731451235976\n",
      "\n",
      "################################################################################################\n",
      "\n",
      "Training model for MSLR-WEB10K/Fold2/\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25623\n",
      "[LightGBM] [Info] Number of data points in the train set: 716683, number of used features: 136\n",
      "#################################  EVALUATING MODEL-FOLD 2   #################################\n",
      "nDCG@3: 0.4538895365009714\n",
      "nDCG@5: 0.4573292117374164\n",
      "nDCG@10: 0.4767546810011047\n",
      "\n",
      "################################################################################################\n",
      "\n",
      "Training model for MSLR-WEB10K/Fold3/\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25659\n",
      "[LightGBM] [Info] Number of data points in the train set: 719111, number of used features: 136\n",
      "#################################  EVALUATING MODEL-FOLD 3   #################################\n",
      "nDCG@3: 0.4490681494620125\n",
      "nDCG@5: 0.4583480538865081\n",
      "nDCG@10: 0.47589507831078093\n",
      "\n",
      "################################################################################################\n",
      "\n",
      "Training model for MSLR-WEB10K/Fold4/\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25631\n",
      "[LightGBM] [Info] Number of data points in the train set: 718768, number of used features: 136\n",
      "#################################  EVALUATING MODEL-FOLD 4   #################################\n",
      "nDCG@3: 0.461178820507814\n",
      "nDCG@5: 0.4663860127875315\n",
      "nDCG@10: 0.487724614983737\n",
      "\n",
      "################################################################################################\n",
      "\n",
      "Training model for MSLR-WEB10K/Fold5/\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25501\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 136\n",
      "#################################  EVALUATING MODEL-FOLD 5   #################################\n",
      "nDCG@3: 0.46963442883961365\n",
      "nDCG@5: 0.4714315145908388\n",
      "nDCG@10: 0.49035928048966515\n",
      "\n",
      "################################################################################################\n"
     ]
    }
   ],
   "source": [
    "def train_lightgbm_model(X_train, y_train, qid_train):\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [3, 5, 10],\n",
    "        # Add other parameters as needed\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, group=np.unique(qid_train, return_counts=True)[1])\n",
    "\n",
    "    model = lgb.train(params, train_data)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to evaluate model on test set and print nDCG scores for multiple k values\n",
    "def evaluate_model(model, X_test, y_test, qid_test, fold_number, k_values=[3, 5, 10]):\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"#################################  EVALUATING MODEL-FOLD {}   #################################\".format(fold_number))\n",
    "\n",
    "    for k in k_values:\n",
    "        ndcg_at_k = compute_ndcg_all(model, X_test, y_test, qid_test, k)\n",
    "        print(f\"nDCG@{k}: {ndcg_at_k}\")\n",
    "    print(\"\\n################################################################################################\")\n",
    "\n",
    "# Train and evaluate LightGBM models for each fold\n",
    "mslr_data_path = \"MSLR-WEB10K\"\n",
    "for fold_number in range(1, 6):\n",
    "    fold_path = f\"{mslr_data_path}/Fold{fold_number}/\"\n",
    "    print(f\"\\nTraining model for {fold_path}\")\n",
    "    \n",
    "    # Load one fold of MSLR data\n",
    "    X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test = load_one_fold(fold_path)\n",
    "\n",
    "    # Train a LightGBM model\n",
    "    model = train_lightgbm_model(X_train, y_train, qid_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    evaluate_model(model, X_test, y_test, qid_test,fold_number,k_values=[3, 5, 10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 14\n",
    "\n",
    "| Evaluation Metric (on test set) | Fold 1 | Fold 2 | Fold 3 | Fold 4 | Fold 5 |\n",
    "|-------------------|--------|--------|--------|--------|--------|\n",
    "| nDCG@3            | 0.456  | 0.454  | 0.449  | 0.461  | 0.470  |\n",
    "| nDCG@5            | 0.463  | 0.457  | 0.458  | 0.466  | 0.471  |\n",
    "| nDCG@10           | 0.483  | 0.477  | 0.476  | 0.488  | 0.490  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for MSLR-WEB10K/Fold1/\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25637\n",
      "[LightGBM] [Info] Number of data points in the train set: 723412, number of used features: 136\n",
      "Top 5 important features for Fold 1:\n",
      "1. Feature 134\n",
      "2. Feature 8\n",
      "3. Feature 108\n",
      "4. Feature 55\n",
      "5. Feature 130\n",
      "#########################################################################\n",
      "\n",
      "Training model for MSLR-WEB10K/Fold2/\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25623\n",
      "[LightGBM] [Info] Number of data points in the train set: 716683, number of used features: 136\n",
      "Top 5 important features for Fold 2:\n",
      "1. Feature 134\n",
      "2. Feature 8\n",
      "3. Feature 55\n",
      "4. Feature 108\n",
      "5. Feature 130\n",
      "#########################################################################\n",
      "\n",
      "Training model for MSLR-WEB10K/Fold3/\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25659\n",
      "[LightGBM] [Info] Number of data points in the train set: 719111, number of used features: 136\n",
      "Top 5 important features for Fold 3:\n",
      "1. Feature 134\n",
      "2. Feature 55\n",
      "3. Feature 108\n",
      "4. Feature 130\n",
      "5. Feature 8\n",
      "#########################################################################\n",
      "\n",
      "Training model for MSLR-WEB10K/Fold4/\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25631\n",
      "[LightGBM] [Info] Number of data points in the train set: 718768, number of used features: 136\n",
      "Top 5 important features for Fold 4:\n",
      "1. Feature 134\n",
      "2. Feature 8\n",
      "3. Feature 55\n",
      "4. Feature 130\n",
      "5. Feature 129\n",
      "#########################################################################\n",
      "\n",
      "Training model for MSLR-WEB10K/Fold5/\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25501\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 136\n",
      "Top 5 important features for Fold 5:\n",
      "1. Feature 134\n",
      "2. Feature 8\n",
      "3. Feature 55\n",
      "4. Feature 108\n",
      "5. Feature 130\n",
      "#########################################################################\n"
     ]
    }
   ],
   "source": [
    "# Function to get the top N important features based on 'gain'\n",
    "def get_top_n_features(model, n=5):\n",
    "    importance_type = 'gain'\n",
    "    feature_importance = get_feature_importance(model, importance_type)\n",
    "    top_indices = np.argsort(feature_importance)[::-1][:n]\n",
    "    return top_indices\n",
    "\n",
    "# Analyze and interpret results for each fold\n",
    "for fold_number in range(1, 6):\n",
    "    fold_path = f\"{mslr_data_path}/Fold{fold_number}/\"\n",
    "    print(f\"\\nTraining model for {fold_path}\")\n",
    "    \n",
    "    # Load one fold of MSLR data\n",
    "    X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test = load_one_fold(fold_path)\n",
    "\n",
    "    # Train a LightGBM model\n",
    "    model = train_lightgbm_model(X_train, y_train, qid_train)\n",
    "\n",
    "    # Get top 5 important features based on 'gain'\n",
    "    top_features = get_top_n_features(model, n=5)\n",
    "    \n",
    "    print(f\"Top 5 important features for Fold {fold_number}:\")\n",
    "    for i, feature_index in enumerate(top_features):\n",
    "        print(f\"{i + 1}. Feature {feature_index + 1}\")\n",
    "    print(\"#########################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 15\n",
    "\n",
    "| Fold # | Top Feature 1 | Top Feature 2 | Top Feature 3 | Top Feature 4 | Top Feature 5 |\n",
    "|------|---------------|---------------|---------------|---------------|---------------|\n",
    "| 1    | 134           | 8             | 108           | 55            | 130           |\n",
    "| 2    | 134           | 8             | 55            | 108           | 130           |\n",
    "| 3    | 134           | 55            | 108           | 130           | 8             |\n",
    "| 4    | 134           | 8             | 55            | 130           | 129           |\n",
    "| 5    | 134           | 8             | 55            | 108           | 130           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing top 20 and removing bottom 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************************************************\n",
      "\n",
      "\n",
      "Analyzing results for Fold 1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25637\n",
      "[LightGBM] [Info] Number of data points in the train set: 723412, number of used features: 136\n",
      "REMOVING TOP 20 FEATURES for MSLR-WEB10K/Fold1/\n",
      "\n",
      "--------------------------Training for MSLR-WEB10K/Fold1/ (Removing TOP 20 features)-----------------------------:\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21582\n",
      "[LightGBM] [Info] Number of data points in the train set: 723412, number of used features: 116\n",
      "#################################  EVALUATING MODEL-FOLD 1   #################################\n",
      "nDCG@10: 0.4083636029390886\n",
      "\n",
      "################################################################################################\n",
      "REMOVING BOTTOM 60 FEATURES for MSLR-WEB10K/Fold1/\n",
      "\n",
      "--------------------------Training for MSLR-WEB10K/Fold1/  (Removing BOTTOM 60 features)-----------------------------:\n",
      "\n",
      "**********************************************************************************************************\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13014\n",
      "[LightGBM] [Info] Number of data points in the train set: 723412, number of used features: 76\n",
      "#################################  EVALUATING MODEL-FOLD 1   #################################\n",
      "nDCG@10: 0.3761216118110393\n",
      "\n",
      "################################################################################################\n",
      "**********************************************************************************************************\n",
      "\n",
      "\n",
      "Analyzing results for Fold 2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25623\n",
      "[LightGBM] [Info] Number of data points in the train set: 716683, number of used features: 136\n",
      "REMOVING TOP 20 FEATURES for MSLR-WEB10K/Fold2/\n",
      "\n",
      "--------------------------Training for MSLR-WEB10K/Fold2/ (Removing TOP 20 features)-----------------------------:\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21551\n",
      "[LightGBM] [Info] Number of data points in the train set: 716683, number of used features: 116\n",
      "#################################  EVALUATING MODEL-FOLD 2   #################################\n",
      "nDCG@10: 0.4045026694861529\n",
      "\n",
      "################################################################################################\n",
      "REMOVING BOTTOM 60 FEATURES for MSLR-WEB10K/Fold2/\n",
      "\n",
      "--------------------------Training for MSLR-WEB10K/Fold2/  (Removing BOTTOM 60 features)-----------------------------:\n",
      "\n",
      "**********************************************************************************************************\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12229\n",
      "[LightGBM] [Info] Number of data points in the train set: 716683, number of used features: 76\n",
      "#################################  EVALUATING MODEL-FOLD 2   #################################\n",
      "nDCG@10: 0.3724982237661007\n",
      "\n",
      "################################################################################################\n",
      "**********************************************************************************************************\n",
      "\n",
      "\n",
      "Analyzing results for Fold 3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25659\n",
      "[LightGBM] [Info] Number of data points in the train set: 719111, number of used features: 136\n",
      "REMOVING TOP 20 FEATURES for MSLR-WEB10K/Fold3/\n",
      "\n",
      "--------------------------Training for MSLR-WEB10K/Fold3/ (Removing TOP 20 features)-----------------------------:\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21720\n",
      "[LightGBM] [Info] Number of data points in the train set: 719111, number of used features: 116\n",
      "#################################  EVALUATING MODEL-FOLD 3   #################################\n",
      "nDCG@10: 0.4116363812695088\n",
      "\n",
      "################################################################################################\n",
      "REMOVING BOTTOM 60 FEATURES for MSLR-WEB10K/Fold3/\n",
      "\n",
      "--------------------------Training for MSLR-WEB10K/Fold3/  (Removing BOTTOM 60 features)-----------------------------:\n",
      "\n",
      "**********************************************************************************************************\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12248\n",
      "[LightGBM] [Info] Number of data points in the train set: 719111, number of used features: 76\n",
      "#################################  EVALUATING MODEL-FOLD 3   #################################\n",
      "nDCG@10: 0.3744687542130175\n",
      "\n",
      "################################################################################################\n",
      "**********************************************************************************************************\n",
      "\n",
      "\n",
      "Analyzing results for Fold 4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25631\n",
      "[LightGBM] [Info] Number of data points in the train set: 718768, number of used features: 136\n",
      "REMOVING TOP 20 FEATURES for MSLR-WEB10K/Fold4/\n",
      "\n",
      "--------------------------Training for MSLR-WEB10K/Fold4/ (Removing TOP 20 features)-----------------------------:\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21670\n",
      "[LightGBM] [Info] Number of data points in the train set: 718768, number of used features: 116\n",
      "#################################  EVALUATING MODEL-FOLD 4   #################################\n",
      "nDCG@10: 0.4121071637228934\n",
      "\n",
      "################################################################################################\n",
      "REMOVING BOTTOM 60 FEATURES for MSLR-WEB10K/Fold4/\n",
      "\n",
      "--------------------------Training for MSLR-WEB10K/Fold4/  (Removing BOTTOM 60 features)-----------------------------:\n",
      "\n",
      "**********************************************************************************************************\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12917\n",
      "[LightGBM] [Info] Number of data points in the train set: 718768, number of used features: 76\n",
      "#################################  EVALUATING MODEL-FOLD 4   #################################\n",
      "nDCG@10: 0.3764053801895373\n",
      "\n",
      "################################################################################################\n",
      "**********************************************************************************************************\n",
      "\n",
      "\n",
      "Analyzing results for Fold 5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25501\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 136\n",
      "REMOVING TOP 20 FEATURES for MSLR-WEB10K/Fold5/\n",
      "\n",
      "--------------------------Training for MSLR-WEB10K/Fold5/ (Removing TOP 20 features)-----------------------------:\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21348\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 116\n",
      "#################################  EVALUATING MODEL-FOLD 5   #################################\n",
      "nDCG@10: 0.4166871494621703\n",
      "\n",
      "################################################################################################\n",
      "REMOVING BOTTOM 60 FEATURES for MSLR-WEB10K/Fold5/\n",
      "\n",
      "--------------------------Training for MSLR-WEB10K/Fold5/  (Removing BOTTOM 60 features)-----------------------------:\n",
      "\n",
      "**********************************************************************************************************\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13062\n",
      "[LightGBM] [Info] Number of data points in the train set: 722602, number of used features: 76\n",
      "#################################  EVALUATING MODEL-FOLD 5   #################################\n",
      "nDCG@10: 0.3795631808598626\n",
      "\n",
      "################################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Function to remove top N features based on 'gain'\n",
    "def remove_top_n_features(X, top_indices, n=20):\n",
    "    X_csc = X.tocsc()\n",
    "    indices_to_keep = np.setdiff1d(np.arange(X.shape[1]), top_indices[:n])\n",
    "    return X_csc[:, indices_to_keep]\n",
    "\n",
    "# Function to remove bottom N features based on 'gain'\n",
    "def remove_bottom_n_features(X, top_indices, n=60):\n",
    "    X_csc = X.tocsc()\n",
    "    indices_to_keep = np.setdiff1d(np.arange(X.shape[1]), top_indices[-n:])\n",
    "    return X_csc[:, indices_to_keep]\n",
    "\n",
    "for fold_number in range(1, 6):\n",
    "    fold_path = f\"{mslr_data_path}/Fold{fold_number}/\"\n",
    "    print(\"**********************************************************************************************************\\n\")\n",
    "    print(f\"\\nAnalyzing results for Fold {fold_number}\")\n",
    "    \n",
    "    # Load one fold of MSLR data\n",
    "    X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test = load_one_fold(fold_path)\n",
    "\n",
    "    # Train a LightGBM model\n",
    "    model = train_lightgbm_model(X_train, y_train, qid_train)\n",
    "\n",
    "    # Get top 20 important features based on 'gain'\n",
    "    top_features = get_top_n_features(model, n=20)\n",
    "\n",
    "    # Remove top 20 features and train a new model\n",
    "    X_train_removed_top = remove_top_n_features(X_train, top_features, n=20)\n",
    "    X_test_removed_top = remove_top_n_features(X_test, top_features, n=20)\n",
    "    print(f\"REMOVING TOP 20 FEATURES for {fold_path}\\n\")\n",
    "    print(f\"--------------------------Training for {fold_path} (Removing TOP 20 features)-----------------------------:\")\n",
    "    new_model_top_removed = train_lightgbm_model(X_train_removed_top, y_train, qid_train)\n",
    "    evaluate_model(new_model_top_removed, X_test_removed_top, y_test, qid_test,fold_number,k_values=[10])\n",
    "    # Get top 60 least important features based on 'gain'\n",
    "    least_features = get_top_n_features(model, n=60)\n",
    "\n",
    "    # Remove bottom 60 features and train a new model\n",
    "    X_train_removed_bottom = remove_bottom_n_features(X_train, least_features, n=60)\n",
    "    X_test_removed_bottom = remove_bottom_n_features(X_test, least_features, n=60)\n",
    "    print(f\"REMOVING BOTTOM 60 FEATURES for {fold_path}\\n\")\n",
    "\n",
    "    print(f\"--------------------------Training for {fold_path}  (Removing BOTTOM 60 features)-----------------------------:\")\n",
    "    print(\"\\n**********************************************************************************************************\")\n",
    "    new_model_bottom_removed = train_lightgbm_model(X_train_removed_bottom, y_train, qid_train)\n",
    "    evaluate_model(new_model_bottom_removed, X_test_removed_bottom, y_test, qid_test,fold_number,k_values=[10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 16\n",
    "\n",
    "• Remove top 20 features:\n",
    "\n",
    "| Fold # | nDCG@10          |\n",
    "|------|------------------|\n",
    "| 1    | 0.408            |\n",
    "| 2    | 0.405            |\n",
    "| 3    | 0.412            |\n",
    "| 4    | 0.412            |\n",
    "| 5    | 0.417            |\n",
    "\n",
    "\n",
    "We note that the performance measured by nDCG@10 shows a decrease in values compared to the original model by around 5%, indicating that these features play a role in the model's predictive capabilities. But the decrease is not much. The reasons could be probably owing to redundancy in features: the removed features might be redundant or highly correlated with other features in the dataset. In such cases, the model can still rely on the correlated information provided by the remaining features. It may also be the case that the LightGBM model may have enough capacity to compensate for the removal of a few top features. The model may have learned alternative patterns from the remaining features.\n",
    "\n",
    "\n",
    "• Remove bottom 60 features:\n",
    "\n",
    "\n",
    "| Fold | nDCG@10          |\n",
    "|------|------------------|\n",
    "| 1    | 0.376            |\n",
    "| 2    | 0.372            |\n",
    "| 3    | 0.374            |\n",
    "| 4    | 0.376            |\n",
    "| 5    | 0.380            |\n",
    "\n",
    "\n",
    "Removing least 60 important features leads to close to an 7 % decrement in performance. This clearly makes sense with the intuition that lower importance features don't contribute much to the model performance, with features being highly redundant. The removed features might be redundant or less informative, and their exclusion did not lead to a loss of crucial information. With respect to the model on removing top 20, it only caused a further 2% loss which is miniscule even after removing 60 feature vectors and hence we note that only the top few features are very important in a model's decision mechanism.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
